\hypertarget{namespacehyperparameters}{}\doxysection{hyperparameters Namespace Reference}
\label{namespacehyperparameters}\index{hyperparameters@{hyperparameters}}
\doxysubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \mbox{\hyperlink{namespacehyperparameters_ac50e58b27c8ececefe9b528373d4dcc8}{mse\+\_\+test}} (model, train\+\_\+features, train\+\_\+target, test\+\_\+features, test\+\_\+target, epoch\+\_\+no, batch\+\_\+size, epoch\+\_\+per\+\_\+fit=10)
\item 
def \mbox{\hyperlink{namespacehyperparameters_ab5df2e4733321acf5773ccacc9fd2b91}{hyper\+\_\+analysis}} (dataset, features, n\+\_\+layers=3, n\+\_\+neurons\+\_\+per\+\_\+layer=100, batch\+\_\+size=10, n\+\_\+epochs=2000, activation=\textquotesingle{}relu\textquotesingle{}, optimizer=\textquotesingle{}Adamax\textquotesingle{}, loss=\textquotesingle{}mean\+\_\+squared\+\_\+error\textquotesingle{}, frac=0.\+5)
\end{DoxyCompactItemize}
\doxysubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{namespacehyperparameters_a657ee9991b4e1de7bf6396412d0b43dd}\label{namespacehyperparameters_a657ee9991b4e1de7bf6396412d0b43dd}} 
{\bfseries dataset} = pash\+\_\+to\+\_\+dataframe(\char`\"{}data/pash/large\+\_\+pash.\+dat\char`\"{})
\item 
\mbox{\Hypertarget{namespacehyperparameters_a4350106837e18649368a3afcf40307c4}\label{namespacehyperparameters_a4350106837e18649368a3afcf40307c4}} 
list {\bfseries features} = \mbox{[}\char`\"{}epsilon\char`\"{}, \char`\"{}a3\char`\"{}\mbox{]}
\item 
\mbox{\Hypertarget{namespacehyperparameters_a55f17330637c4a217c8fb22e331ac6cb}\label{namespacehyperparameters_a55f17330637c4a217c8fb22e331ac6cb}} 
{\bfseries loss\+\_\+train\+\_\+epoch}
\item 
\mbox{\Hypertarget{namespacehyperparameters_ab486b325a69998908d53b6beda38c997}\label{namespacehyperparameters_ab486b325a69998908d53b6beda38c997}} 
{\bfseries loss\+\_\+test\+\_\+epoch}
\item 
\mbox{\Hypertarget{namespacehyperparameters_aded7c203e0a34a53e9f8b66f43853c7a}\label{namespacehyperparameters_aded7c203e0a34a53e9f8b66f43853c7a}} 
{\bfseries loss\+\_\+train\+\_\+hp}
\item 
\mbox{\Hypertarget{namespacehyperparameters_a222c6828f10d753c96a595a9112d9f94}\label{namespacehyperparameters_a222c6828f10d753c96a595a9112d9f94}} 
{\bfseries loss\+\_\+test\+\_\+hp}
\item 
\mbox{\Hypertarget{namespacehyperparameters_a7e0686d80d0423763ba271ccda3f715b}\label{namespacehyperparameters_a7e0686d80d0423763ba271ccda3f715b}} 
{\bfseries n\+\_\+neurons\+\_\+per\+\_\+layer}
\item 
\mbox{\Hypertarget{namespacehyperparameters_adbb8d88875cc0e37e93702168736d5e3}\label{namespacehyperparameters_adbb8d88875cc0e37e93702168736d5e3}} 
{\bfseries n\+\_\+layers}
\item 
\mbox{\Hypertarget{namespacehyperparameters_a08d12a29e170931823e8b02e4ab79b32}\label{namespacehyperparameters_a08d12a29e170931823e8b02e4ab79b32}} 
{\bfseries n\+\_\+epochs}
\item 
\mbox{\Hypertarget{namespacehyperparameters_aa913bda1aacd7f7dfa7937408928cc5b}\label{namespacehyperparameters_aa913bda1aacd7f7dfa7937408928cc5b}} 
{\bfseries frac}
\item 
\mbox{\Hypertarget{namespacehyperparameters_ade2621fd0037aba9483203110fde87d8}\label{namespacehyperparameters_ade2621fd0037aba9483203110fde87d8}} 
{\bfseries optimizer} = \mbox{[}\textquotesingle{}SGD\textquotesingle{}, \textquotesingle{}RMSprop\textquotesingle{}, \textquotesingle{}Adam\textquotesingle{}, \textquotesingle{}Adadelta\textquotesingle{}, \textquotesingle{}Adagrad\textquotesingle{}, \textquotesingle{}Adamax\textquotesingle{}, \textquotesingle{}Nadam\textquotesingle{}, \textquotesingle{}Ftrl\textquotesingle{}\mbox{]}
\item 
\mbox{\Hypertarget{namespacehyperparameters_ae310f5c0a4fcbef53ba9b49cf2e86afe}\label{namespacehyperparameters_ae310f5c0a4fcbef53ba9b49cf2e86afe}} 
list {\bfseries opt} = \mbox{[}1, 2, 3, 4, 5, 6, 7, 8\mbox{]}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Function to compare hyperparameters
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\mbox{\Hypertarget{namespacehyperparameters_ab5df2e4733321acf5773ccacc9fd2b91}\label{namespacehyperparameters_ab5df2e4733321acf5773ccacc9fd2b91}} 
\index{hyperparameters@{hyperparameters}!hyper\_analysis@{hyper\_analysis}}
\index{hyper\_analysis@{hyper\_analysis}!hyperparameters@{hyperparameters}}
\doxysubsubsection{\texorpdfstring{hyper\_analysis()}{hyper\_analysis()}}
{\footnotesize\ttfamily def hyperparameters.\+hyper\+\_\+analysis (\begin{DoxyParamCaption}\item[{}]{dataset,  }\item[{}]{features,  }\item[{}]{n\+\_\+layers = {\ttfamily 3},  }\item[{}]{n\+\_\+neurons\+\_\+per\+\_\+layer = {\ttfamily 100},  }\item[{}]{batch\+\_\+size = {\ttfamily 10},  }\item[{}]{n\+\_\+epochs = {\ttfamily 2000},  }\item[{}]{activation = {\ttfamily \textquotesingle{}relu\textquotesingle{}},  }\item[{}]{optimizer = {\ttfamily \textquotesingle{}Adamax\textquotesingle{}},  }\item[{}]{loss = {\ttfamily \textquotesingle{}mean\+\_\+squared\+\_\+error\textquotesingle{}},  }\item[{}]{frac = {\ttfamily 0.5} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Takes dataset, keys of features and values for hyperparameters (takes a default value when no input given) and
returns the loss on training set and the loss on test set per epoch
and also the final loss reached as a function of the varied hyperparameters
:param dataset: (pandas DataFrame) initial DataFrame
:param features: (str list) list of keys of features
:param n_layers: (default int or int list) list of different numbers of layers to be tested or 3 by default
:param n_neurons_per_layer: (default int or int list) list of different numbers of neurons per layer to be tested
or 100 by default
:param batch_size: (default int or int list) list of different numbers of batch sizes to be tested or 10 by default
:param n_epochs: (default int or int list) list of different numbers of epochs to be tested or 2000 by default
:param activation: (default str or str list) list of different activation functions to be tested
or 'relu' by default
:param optimizer: (default str or str list) list of different optimizers to be tested or 'adamax' by default
:param loss: (default str or str list) list of different loss functions to be tested
or 'mean_squared_error' by default
:param frac: (float) fraction of the data turned into training set - half by default
:return losses_train_epoch: (float list) list of losses for training set per epoch
:return losses_test_epoch: (float list) list of losses for test set per epoch
:return losses_train_hp: (float list) list of final losses for training set for different values of a hyperparameter
:return losses_test_hp: (float list) list of final losses for test set for different values of a hyperparameter
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacehyperparameters_ac50e58b27c8ececefe9b528373d4dcc8}\label{namespacehyperparameters_ac50e58b27c8ececefe9b528373d4dcc8}} 
\index{hyperparameters@{hyperparameters}!mse\_test@{mse\_test}}
\index{mse\_test@{mse\_test}!hyperparameters@{hyperparameters}}
\doxysubsubsection{\texorpdfstring{mse\_test()}{mse\_test()}}
{\footnotesize\ttfamily def hyperparameters.\+mse\+\_\+test (\begin{DoxyParamCaption}\item[{}]{model,  }\item[{}]{train\+\_\+features,  }\item[{}]{train\+\_\+target,  }\item[{}]{test\+\_\+features,  }\item[{}]{test\+\_\+target,  }\item[{}]{epoch\+\_\+no,  }\item[{}]{batch\+\_\+size,  }\item[{}]{epoch\+\_\+per\+\_\+fit = {\ttfamily 10} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Takes a model, training datasets, test datasets, number of epochs and a batch size
and returns a list of losses on training and test dataset calculated every epoch_per_fit epochs.
:param model: (keras model) compiled model
:param train_features: (pandas DataFrame) training features
:param train_target: (pandas DataFrame) training targets
:param test_features: (pandas DataFrame) test features
:param test_target: (pandas DataFrame) test target
:param epoch_no: (int) total number of epochs
:param batch_size: (int) batch_size parameter for fit
:param epoch_per_fit: (int) number of epochs for every fit at which point we obtain losses - 10 by default
:return losses: (float list) list of losses on training set
:return losses_test: (float list) list of losses on test set
\end{DoxyVerb}
 